{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv('Cancer_Classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>benign_0__mal_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  \\\n",
       "0                  0.2654          0.4601                  0.11890   \n",
       "1                  0.1860          0.2750                  0.08902   \n",
       "2                  0.2430          0.3613                  0.08758   \n",
       "3                  0.2575          0.6638                  0.17300   \n",
       "4                  0.1625          0.2364                  0.07678   \n",
       "..                    ...             ...                      ...   \n",
       "564                0.2216          0.2060                  0.07115   \n",
       "565                0.1628          0.2572                  0.06637   \n",
       "566                0.1418          0.2218                  0.07820   \n",
       "567                0.2650          0.4087                  0.12400   \n",
       "568                0.0000          0.2871                  0.07039   \n",
       "\n",
       "     benign_0__mal_1  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "..               ...  \n",
       "564                0  \n",
       "565                0  \n",
       "566                0  \n",
       "567                0  \n",
       "568                1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Data.drop('benign_0__mal_1',axis=1).values\n",
    "y = Data['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "Scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=Scaler.fit_transform(x_train)\n",
    "x_test=Scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30280346, 0.28779168, 0.28292922, ..., 0.35910653, 0.13207175,\n",
       "        0.09320478],\n",
       "       [0.27589412, 0.25160636, 0.25735448, ..., 0.33852234, 0.33609304,\n",
       "        0.11976912],\n",
       "       [0.38255296, 0.26885357, 0.36398575, ..., 0.28549828, 0.40114331,\n",
       "        0.11681753],\n",
       "       ...,\n",
       "       [0.44468907, 0.3361515 , 0.42142408, ..., 0.52233677, 0.21387739,\n",
       "        0.05791683],\n",
       "       [0.57678947, 0.39634765, 0.55279156, ..., 0.70378007, 0.41671595,\n",
       "        0.2863702 ],\n",
       "       [0.32922354, 0.30334799, 0.31402418, ..., 0.44123711, 0.25310467,\n",
       "        0.23133937]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FOR A BINARY CLASSIFICATION PROBLEM:\n",
    "Model.complile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = Sequential()\n",
    "Model.add(Dense(units=30,activation='relu',kernel_initializer='he_uniform'))\n",
    "Model.add(Dense(units=15,activation='relu',kernel_initializer='he_uniform'))\n",
    "\n",
    "Model.add(Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform'))\n",
    "\n",
    "Model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 2s 4ms/sample - loss: 0.6829 - val_loss: 0.6492\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 289us/sample - loss: 0.6269 - val_loss: 0.6050\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 307us/sample - loss: 0.5811 - val_loss: 0.5618\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 308us/sample - loss: 0.5370 - val_loss: 0.5164\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.4936 - val_loss: 0.4716\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 184us/sample - loss: 0.4486 - val_loss: 0.4275\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 288us/sample - loss: 0.4076 - val_loss: 0.3864\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.3672 - val_loss: 0.3466\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.3317 - val_loss: 0.3098\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 227us/sample - loss: 0.2998 - val_loss: 0.2811\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 190us/sample - loss: 0.2790 - val_loss: 0.2566\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 188us/sample - loss: 0.2552 - val_loss: 0.2414\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 180us/sample - loss: 0.2380 - val_loss: 0.2215\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 187us/sample - loss: 0.2226 - val_loss: 0.2108\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.2093 - val_loss: 0.1956\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.1990 - val_loss: 0.1896\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.1870 - val_loss: 0.1791\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 278us/sample - loss: 0.1785 - val_loss: 0.1720\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.1704 - val_loss: 0.1654\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 200us/sample - loss: 0.1642 - val_loss: 0.1621\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.1604 - val_loss: 0.1563\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.1508 - val_loss: 0.1546\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.1445 - val_loss: 0.1480\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.1392 - val_loss: 0.1476\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.1350 - val_loss: 0.1437\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.1298 - val_loss: 0.1386\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 281us/sample - loss: 0.1255 - val_loss: 0.1377\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 300us/sample - loss: 0.1206 - val_loss: 0.1349\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 262us/sample - loss: 0.1172 - val_loss: 0.1335\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 280us/sample - loss: 0.1127 - val_loss: 0.1299\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 278us/sample - loss: 0.1101 - val_loss: 0.1282\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 315us/sample - loss: 0.1078 - val_loss: 0.1281\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 266us/sample - loss: 0.1096 - val_loss: 0.1290\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.1029 - val_loss: 0.1258\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 301us/sample - loss: 0.1005 - val_loss: 0.1248\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 294us/sample - loss: 0.0959 - val_loss: 0.1263\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 330us/sample - loss: 0.0933 - val_loss: 0.1220\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 324us/sample - loss: 0.0911 - val_loss: 0.1258\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0895 - val_loss: 0.1198\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.0877 - val_loss: 0.1209\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 175us/sample - loss: 0.0847 - val_loss: 0.1202\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 205us/sample - loss: 0.0835 - val_loss: 0.1229\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0824 - val_loss: 0.1188\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0809 - val_loss: 0.1181\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0793 - val_loss: 0.1190\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 181us/sample - loss: 0.0775 - val_loss: 0.1223\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0767 - val_loss: 0.1191\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0749 - val_loss: 0.1197\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.0765 - val_loss: 0.1190\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 198us/sample - loss: 0.0802 - val_loss: 0.1202\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.0773 - val_loss: 0.1180\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0709 - val_loss: 0.1187\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0698 - val_loss: 0.1182\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0700 - val_loss: 0.1218\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 150us/sample - loss: 0.0702 - val_loss: 0.1170\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 368us/sample - loss: 0.0676 - val_loss: 0.1175\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0677 - val_loss: 0.1213\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0672 - val_loss: 0.1173\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 150us/sample - loss: 0.0678 - val_loss: 0.1158\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 304us/sample - loss: 0.0643 - val_loss: 0.1228\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 271us/sample - loss: 0.0651 - val_loss: 0.1197\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 267us/sample - loss: 0.0638 - val_loss: 0.1175\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 289us/sample - loss: 0.0634 - val_loss: 0.1219\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 273us/sample - loss: 0.0628 - val_loss: 0.1170\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 266us/sample - loss: 0.0633 - val_loss: 0.1189\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 295us/sample - loss: 0.0644 - val_loss: 0.1200\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 306us/sample - loss: 0.0615 - val_loss: 0.1223\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 290us/sample - loss: 0.0617 - val_loss: 0.1192\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 330us/sample - loss: 0.0608 - val_loss: 0.1214\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 310us/sample - loss: 0.0619 - val_loss: 0.1222\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.0654 - val_loss: 0.1177\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 183us/sample - loss: 0.0600 - val_loss: 0.1292\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.0608 - val_loss: 0.1185\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 148us/sample - loss: 0.0639 - val_loss: 0.1294\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0613 - val_loss: 0.1211\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0584 - val_loss: 0.1190\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0594 - val_loss: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 187us/sample - loss: 0.0574 - val_loss: 0.1196\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.0591 - val_loss: 0.1196\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0564 - val_loss: 0.1271\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 170us/sample - loss: 0.0577 - val_loss: 0.1221\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0569 - val_loss: 0.1190\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0560 - val_loss: 0.1216\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0555 - val_loss: 0.1274\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.0574 - val_loss: 0.1240\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.0554 - val_loss: 0.1190\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.0545 - val_loss: 0.1251\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0559 - val_loss: 0.1213\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0544 - val_loss: 0.1233\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0543 - val_loss: 0.1256\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0550 - val_loss: 0.1215\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0553 - val_loss: 0.1218\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0538 - val_loss: 0.1221\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0548 - val_loss: 0.1207\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0559 - val_loss: 0.1288\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 252us/sample - loss: 0.0533 - val_loss: 0.1288\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0523 - val_loss: 0.1208\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 226us/sample - loss: 0.0537 - val_loss: 0.1253\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0527 - val_loss: 0.1298\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 208us/sample - loss: 0.0548 - val_loss: 0.1218\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 264us/sample - loss: 0.0537 - val_loss: 0.1285\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 322us/sample - loss: 0.0526 - val_loss: 0.1317\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0541 - val_loss: 0.1245\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 177us/sample - loss: 0.0525 - val_loss: 0.1309\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 186us/sample - loss: 0.0544 - val_loss: 0.1239\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0555 - val_loss: 0.1311\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0555 - val_loss: 0.1249\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.0511 - val_loss: 0.1285\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0513 - val_loss: 0.1208\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0528 - val_loss: 0.1289\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 248us/sample - loss: 0.0513 - val_loss: 0.1237\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 275us/sample - loss: 0.0509 - val_loss: 0.1327\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 278us/sample - loss: 0.0521 - val_loss: 0.1271\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.0510 - val_loss: 0.1344\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.0509 - val_loss: 0.1240\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0531 - val_loss: 0.1364\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0563 - val_loss: 0.1233\n",
      "Epoch 118/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.0487 - val_loss: 0.1477\n",
      "Epoch 119/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0507 - val_loss: 0.1203\n",
      "Epoch 120/600\n",
      "426/426 [==============================] - 0s 209us/sample - loss: 0.0497 - val_loss: 0.1383\n",
      "Epoch 121/600\n",
      "426/426 [==============================] - 0s 226us/sample - loss: 0.0498 - val_loss: 0.1312\n",
      "Epoch 122/600\n",
      "426/426 [==============================] - 0s 180us/sample - loss: 0.0482 - val_loss: 0.1308\n",
      "Epoch 123/600\n",
      "426/426 [==============================] - 0s 188us/sample - loss: 0.0488 - val_loss: 0.1288\n",
      "Epoch 124/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.0487 - val_loss: 0.1302\n",
      "Epoch 125/600\n",
      "426/426 [==============================] - 0s 275us/sample - loss: 0.0486 - val_loss: 0.1338\n",
      "Epoch 126/600\n",
      "426/426 [==============================] - 0s 278us/sample - loss: 0.0481 - val_loss: 0.1265\n",
      "Epoch 127/600\n",
      "426/426 [==============================] - 0s 269us/sample - loss: 0.0483 - val_loss: 0.1272\n",
      "Epoch 128/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0488 - val_loss: 0.1400\n",
      "Epoch 129/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.0492 - val_loss: 0.1246\n",
      "Epoch 130/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.0488 - val_loss: 0.1320\n",
      "Epoch 131/600\n",
      "426/426 [==============================] - 0s 205us/sample - loss: 0.0472 - val_loss: 0.1294\n",
      "Epoch 132/600\n",
      "426/426 [==============================] - 0s 252us/sample - loss: 0.0478 - val_loss: 0.1344\n",
      "Epoch 133/600\n",
      "426/426 [==============================] - 0s 298us/sample - loss: 0.0480 - val_loss: 0.1342\n",
      "Epoch 134/600\n",
      "426/426 [==============================] - 0s 214us/sample - loss: 0.0496 - val_loss: 0.1282\n",
      "Epoch 135/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0516 - val_loss: 0.1365\n",
      "Epoch 136/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.0502 - val_loss: 0.1337\n",
      "Epoch 137/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0463 - val_loss: 0.1253\n",
      "Epoch 138/600\n",
      "426/426 [==============================] - 0s 178us/sample - loss: 0.0479 - val_loss: 0.1331\n",
      "Epoch 139/600\n",
      "426/426 [==============================] - 0s 194us/sample - loss: 0.0475 - val_loss: 0.1282\n",
      "Epoch 140/600\n",
      "426/426 [==============================] - 0s 190us/sample - loss: 0.0487 - val_loss: 0.1348\n",
      "Epoch 141/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0463 - val_loss: 0.1370\n",
      "Epoch 142/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0468 - val_loss: 0.1259\n",
      "Epoch 143/600\n",
      "426/426 [==============================] - 0s 184us/sample - loss: 0.0492 - val_loss: 0.1414\n",
      "Epoch 144/600\n",
      "426/426 [==============================] - 0s 178us/sample - loss: 0.0485 - val_loss: 0.1270\n",
      "Epoch 145/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0490 - val_loss: 0.1400\n",
      "Epoch 146/600\n",
      "426/426 [==============================] - 0s 187us/sample - loss: 0.0494 - val_loss: 0.1320\n",
      "Epoch 147/600\n",
      "426/426 [==============================] - 0s 182us/sample - loss: 0.0453 - val_loss: 0.1415\n",
      "Epoch 148/600\n",
      "426/426 [==============================] - 0s 188us/sample - loss: 0.0468 - val_loss: 0.1323\n",
      "Epoch 149/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0443 - val_loss: 0.1420\n",
      "Epoch 150/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0444 - val_loss: 0.1280\n",
      "Epoch 151/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0469 - val_loss: 0.1397\n",
      "Epoch 152/600\n",
      "426/426 [==============================] - 0s 186us/sample - loss: 0.0455 - val_loss: 0.1351\n",
      "Epoch 153/600\n",
      "426/426 [==============================] - 0s 300us/sample - loss: 0.0446 - val_loss: 0.1352\n",
      "Epoch 154/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0442 - val_loss: 0.1385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0452 - val_loss: 0.1296\n",
      "Epoch 156/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.0442 - val_loss: 0.1358\n",
      "Epoch 157/600\n",
      "426/426 [==============================] - 0s 267us/sample - loss: 0.0447 - val_loss: 0.1358\n",
      "Epoch 158/600\n",
      "426/426 [==============================] - 0s 234us/sample - loss: 0.0469 - val_loss: 0.1337\n",
      "Epoch 159/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0449 - val_loss: 0.1332\n",
      "Epoch 160/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0431 - val_loss: 0.1357\n",
      "Epoch 161/600\n",
      "426/426 [==============================] - 0s 293us/sample - loss: 0.0446 - val_loss: 0.1340\n",
      "Epoch 162/600\n",
      "426/426 [==============================] - 0s 306us/sample - loss: 0.0446 - val_loss: 0.1361\n",
      "Epoch 163/600\n",
      "426/426 [==============================] - 0s 306us/sample - loss: 0.0456 - val_loss: 0.1339\n",
      "Epoch 164/600\n",
      "426/426 [==============================] - 0s 307us/sample - loss: 0.0486 - val_loss: 0.1238\n",
      "Epoch 165/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0458 - val_loss: 0.1331\n",
      "Epoch 166/600\n",
      "426/426 [==============================] - 0s 198us/sample - loss: 0.0454 - val_loss: 0.1374\n",
      "Epoch 167/600\n",
      "426/426 [==============================] - 0s 200us/sample - loss: 0.0428 - val_loss: 0.1355\n",
      "Epoch 168/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0438 - val_loss: 0.1346\n",
      "Epoch 169/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.0431 - val_loss: 0.1388\n",
      "Epoch 170/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0422 - val_loss: 0.1311\n",
      "Epoch 171/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0445 - val_loss: 0.1292\n",
      "Epoch 172/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0439 - val_loss: 0.1348\n",
      "Epoch 173/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0428 - val_loss: 0.1359\n",
      "Epoch 174/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0463 - val_loss: 0.1396\n",
      "Epoch 175/600\n",
      "426/426 [==============================] - 0s 180us/sample - loss: 0.0461 - val_loss: 0.1312\n",
      "Epoch 176/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.0424 - val_loss: 0.1262\n",
      "Epoch 177/600\n",
      "426/426 [==============================] - 0s 337us/sample - loss: 0.0422 - val_loss: 0.1375\n",
      "Epoch 178/600\n",
      "426/426 [==============================] - 0s 146us/sample - loss: 0.0440 - val_loss: 0.1321\n",
      "Epoch 179/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0438 - val_loss: 0.1359\n",
      "Epoch 180/600\n",
      "426/426 [==============================] - 0s 170us/sample - loss: 0.0444 - val_loss: 0.1322\n",
      "Epoch 181/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0434 - val_loss: 0.1413\n",
      "Epoch 182/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0411 - val_loss: 0.1367\n",
      "Epoch 183/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0406 - val_loss: 0.1319\n",
      "Epoch 184/600\n",
      "426/426 [==============================] - 0s 263us/sample - loss: 0.0418 - val_loss: 0.1348\n",
      "Epoch 185/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0407 - val_loss: 0.1349\n",
      "Epoch 186/600\n",
      "426/426 [==============================] - 0s 259us/sample - loss: 0.0406 - val_loss: 0.1342\n",
      "Epoch 187/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0402 - val_loss: 0.1332\n",
      "Epoch 188/600\n",
      "426/426 [==============================] - 0s 262us/sample - loss: 0.0431 - val_loss: 0.1436\n",
      "Epoch 189/600\n",
      "426/426 [==============================] - 0s 313us/sample - loss: 0.0410 - val_loss: 0.1388\n",
      "Epoch 190/600\n",
      "426/426 [==============================] - 0s 283us/sample - loss: 0.0412 - val_loss: 0.1383\n",
      "Epoch 191/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0414 - val_loss: 0.1361\n",
      "Epoch 192/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.0420 - val_loss: 0.1360\n",
      "Epoch 193/600\n",
      "426/426 [==============================] - 0s 266us/sample - loss: 0.0394 - val_loss: 0.1441\n",
      "Epoch 194/600\n",
      "426/426 [==============================] - 0s 267us/sample - loss: 0.0418 - val_loss: 0.1297\n",
      "Epoch 195/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0403 - val_loss: 0.1377\n",
      "Epoch 196/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0397 - val_loss: 0.1422\n",
      "Epoch 197/600\n",
      "426/426 [==============================] - 0s 316us/sample - loss: 0.0397 - val_loss: 0.1372\n",
      "Epoch 198/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.0413 - val_loss: 0.1327\n",
      "Epoch 199/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.0522 - val_loss: 0.1471\n",
      "Epoch 200/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0444 - val_loss: 0.1338\n",
      "Epoch 201/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0400 - val_loss: 0.1354\n",
      "Epoch 202/600\n",
      "426/426 [==============================] - 0s 148us/sample - loss: 0.0428 - val_loss: 0.1380\n",
      "Epoch 203/600\n",
      "426/426 [==============================] - 0s 143us/sample - loss: 0.0485 - val_loss: 0.1426\n",
      "Epoch 204/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0419 - val_loss: 0.1260\n",
      "Epoch 205/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0384 - val_loss: 0.1500\n",
      "Epoch 206/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0415 - val_loss: 0.1344\n",
      "Epoch 207/600\n",
      "426/426 [==============================] - 0s 227us/sample - loss: 0.0448 - val_loss: 0.1360\n",
      "Epoch 208/600\n",
      "426/426 [==============================] - 0s 236us/sample - loss: 0.0473 - val_loss: 0.1386\n",
      "Epoch 209/600\n",
      "426/426 [==============================] - 0s 234us/sample - loss: 0.0391 - val_loss: 0.1347\n",
      "Epoch 210/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.0415 - val_loss: 0.1303\n",
      "Epoch 211/600\n",
      "426/426 [==============================] - 0s 224us/sample - loss: 0.0386 - val_loss: 0.1340\n",
      "Epoch 212/600\n",
      "426/426 [==============================] - 0s 243us/sample - loss: 0.0398 - val_loss: 0.1344\n",
      "Epoch 213/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0385 - val_loss: 0.1353\n",
      "Epoch 214/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.0380 - val_loss: 0.1380\n",
      "Epoch 215/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0376 - val_loss: 0.1372\n",
      "Epoch 216/600\n",
      "426/426 [==============================] - 0s 358us/sample - loss: 0.0381 - val_loss: 0.1414\n",
      "Epoch 217/600\n",
      "426/426 [==============================] - 0s 205us/sample - loss: 0.0375 - val_loss: 0.1387\n",
      "Epoch 218/600\n",
      "426/426 [==============================] - 0s 131us/sample - loss: 0.0377 - val_loss: 0.1384\n",
      "Epoch 219/600\n",
      "426/426 [==============================] - 0s 147us/sample - loss: 0.0381 - val_loss: 0.1328\n",
      "Epoch 220/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0422 - val_loss: 0.1414\n",
      "Epoch 221/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0405 - val_loss: 0.1366\n",
      "Epoch 222/600\n",
      "426/426 [==============================] - 0s 187us/sample - loss: 0.0404 - val_loss: 0.1345\n",
      "Epoch 223/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0387 - val_loss: 0.1340\n",
      "Epoch 224/600\n",
      "426/426 [==============================] - 0s 231us/sample - loss: 0.0378 - val_loss: 0.1352\n",
      "Epoch 225/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 0.0367 - val_loss: 0.1327\n",
      "Epoch 226/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0363 - val_loss: 0.1387\n",
      "Epoch 227/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0370 - val_loss: 0.1429\n",
      "Epoch 228/600\n",
      "426/426 [==============================] - 0s 231us/sample - loss: 0.0373 - val_loss: 0.1362\n",
      "Epoch 229/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0392 - val_loss: 0.1351\n",
      "Epoch 230/600\n",
      "426/426 [==============================] - 0s 276us/sample - loss: 0.0356 - val_loss: 0.1428\n",
      "Epoch 231/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0373 - val_loss: 0.1330\n",
      "Epoch 232/600\n",
      "426/426 [==============================] - 0s 257us/sample - loss: 0.0366 - val_loss: 0.1378\n",
      "Epoch 233/600\n",
      "426/426 [==============================] - 0s 220us/sample - loss: 0.0384 - val_loss: 0.1367\n",
      "Epoch 234/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0370 - val_loss: 0.1341\n",
      "Epoch 235/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0386 - val_loss: 0.1365\n",
      "Epoch 236/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0386 - val_loss: 0.1397\n",
      "Epoch 237/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0355 - val_loss: 0.1346\n",
      "Epoch 238/600\n",
      "426/426 [==============================] - 0s 148us/sample - loss: 0.0352 - val_loss: 0.1313\n",
      "Epoch 239/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0364 - val_loss: 0.1514\n",
      "Epoch 240/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0354 - val_loss: 0.1354\n",
      "Epoch 241/600\n",
      "426/426 [==============================] - 0s 222us/sample - loss: 0.0363 - val_loss: 0.1353\n",
      "Epoch 242/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0344 - val_loss: 0.1410\n",
      "Epoch 243/600\n",
      "426/426 [==============================] - 0s 236us/sample - loss: 0.0369 - val_loss: 0.1327\n",
      "Epoch 244/600\n",
      "426/426 [==============================] - 0s 219us/sample - loss: 0.0349 - val_loss: 0.1404\n",
      "Epoch 245/600\n",
      "426/426 [==============================] - 0s 206us/sample - loss: 0.0354 - val_loss: 0.1484\n",
      "Epoch 246/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0392 - val_loss: 0.1322\n",
      "Epoch 247/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0366 - val_loss: 0.1510\n",
      "Epoch 248/600\n",
      "426/426 [==============================] - 0s 281us/sample - loss: 0.0409 - val_loss: 0.1431\n",
      "Epoch 249/600\n",
      "426/426 [==============================] - 0s 278us/sample - loss: 0.0349 - val_loss: 0.1488\n",
      "Epoch 250/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0354 - val_loss: 0.1364\n",
      "Epoch 251/600\n",
      "426/426 [==============================] - 0s 259us/sample - loss: 0.0346 - val_loss: 0.1450\n",
      "Epoch 252/600\n",
      "426/426 [==============================] - 0s 233us/sample - loss: 0.0341 - val_loss: 0.1372\n",
      "Epoch 253/600\n",
      "426/426 [==============================] - 0s 208us/sample - loss: 0.0369 - val_loss: 0.1513\n",
      "Epoch 254/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0354 - val_loss: 0.1377\n",
      "Epoch 255/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0333 - val_loss: 0.1387\n",
      "Epoch 256/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0338 - val_loss: 0.1512\n",
      "Epoch 257/600\n",
      "426/426 [==============================] - 0s 229us/sample - loss: 0.0368 - val_loss: 0.1461\n",
      "Epoch 258/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0339 - val_loss: 0.1393\n",
      "Epoch 259/600\n",
      "426/426 [==============================] - 0s 285us/sample - loss: 0.0346 - val_loss: 0.1542\n",
      "Epoch 260/600\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.027 - 0s 259us/sample - loss: 0.0326 - val_loss: 0.1355\n",
      "Epoch 261/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0345 - val_loss: 0.1366\n",
      "Epoch 262/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0334 - val_loss: 0.1412\n",
      "Epoch 263/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0337 - val_loss: 0.1405\n",
      "Epoch 264/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0359 - val_loss: 0.1456\n",
      "Epoch 265/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0343 - val_loss: 0.1448\n",
      "Epoch 266/600\n",
      "426/426 [==============================] - 0s 227us/sample - loss: 0.0335 - val_loss: 0.1449\n",
      "Epoch 267/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0325 - val_loss: 0.1422\n",
      "Epoch 268/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.0323 - val_loss: 0.1364\n",
      "Epoch 269/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0327 - val_loss: 0.1501\n",
      "Epoch 270/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0331 - val_loss: 0.1410\n",
      "Epoch 271/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.0323 - val_loss: 0.1359\n",
      "Epoch 272/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0323 - val_loss: 0.1478\n",
      "Epoch 273/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0341 - val_loss: 0.1404\n",
      "Epoch 274/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0385 - val_loss: 0.1473\n",
      "Epoch 275/600\n",
      "426/426 [==============================] - 0s 163us/sample - loss: 0.0341 - val_loss: 0.1409\n",
      "Epoch 276/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0325 - val_loss: 0.1394\n",
      "Epoch 277/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0316 - val_loss: 0.1385\n",
      "Epoch 278/600\n",
      "426/426 [==============================] - 0s 248us/sample - loss: 0.0327 - val_loss: 0.1378\n",
      "Epoch 279/600\n",
      "426/426 [==============================] - 0s 247us/sample - loss: 0.0341 - val_loss: 0.1445\n",
      "Epoch 280/600\n",
      "426/426 [==============================] - 0s 241us/sample - loss: 0.0322 - val_loss: 0.1425\n",
      "Epoch 281/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0315 - val_loss: 0.1405\n",
      "Epoch 282/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.0313 - val_loss: 0.1445\n",
      "Epoch 283/600\n",
      "426/426 [==============================] - 0s 241us/sample - loss: 0.0308 - val_loss: 0.1495\n",
      "Epoch 284/600\n",
      "426/426 [==============================] - 0s 297us/sample - loss: 0.0310 - val_loss: 0.1423\n",
      "Epoch 285/600\n",
      "426/426 [==============================] - 0s 281us/sample - loss: 0.0314 - val_loss: 0.1489\n",
      "Epoch 286/600\n",
      "426/426 [==============================] - 0s 267us/sample - loss: 0.0303 - val_loss: 0.1475\n",
      "Epoch 287/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0309 - val_loss: 0.1474\n",
      "Epoch 288/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0322 - val_loss: 0.1526\n",
      "Epoch 289/600\n",
      "426/426 [==============================] - 0s 206us/sample - loss: 0.0319 - val_loss: 0.1532\n",
      "Epoch 290/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0314 - val_loss: 0.1397\n",
      "Epoch 291/600\n",
      "426/426 [==============================] - 0s 259us/sample - loss: 0.0313 - val_loss: 0.1422\n",
      "Epoch 292/600\n",
      "426/426 [==============================] - 0s 333us/sample - loss: 0.0307 - val_loss: 0.1468\n",
      "Epoch 293/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0304 - val_loss: 0.1410\n",
      "Epoch 294/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0312 - val_loss: 0.1432\n",
      "Epoch 295/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0328 - val_loss: 0.1378\n",
      "Epoch 296/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0333 - val_loss: 0.1403\n",
      "Epoch 297/600\n",
      "426/426 [==============================] - 0s 138us/sample - loss: 0.0298 - val_loss: 0.1400\n",
      "Epoch 298/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0294 - val_loss: 0.1478\n",
      "Epoch 299/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0337 - val_loss: 0.1373\n",
      "Epoch 300/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0356 - val_loss: 0.1436\n",
      "Epoch 301/600\n",
      "426/426 [==============================] - 0s 206us/sample - loss: 0.0306 - val_loss: 0.1465\n",
      "Epoch 302/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0301 - val_loss: 0.1467\n",
      "Epoch 303/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0345 - val_loss: 0.1369\n",
      "Epoch 304/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0299 - val_loss: 0.1446\n",
      "Epoch 305/600\n",
      "426/426 [==============================] - 0s 170us/sample - loss: 0.0302 - val_loss: 0.1420\n",
      "Epoch 306/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0293 - val_loss: 0.1493\n",
      "Epoch 307/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0296 - val_loss: 0.1397\n",
      "Epoch 308/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0282 - val_loss: 0.1457\n",
      "Epoch 309/600\n",
      "426/426 [==============================] - 0s 269us/sample - loss: 0.0283 - val_loss: 0.1425\n",
      "Epoch 310/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.0285 - val_loss: 0.1417\n",
      "Epoch 311/600\n",
      "426/426 [==============================] - 0s 252us/sample - loss: 0.0287 - val_loss: 0.1535\n",
      "Epoch 312/600\n",
      "426/426 [==============================] - 0s 287us/sample - loss: 0.0289 - val_loss: 0.1449\n",
      "Epoch 313/600\n",
      "426/426 [==============================] - 0s 267us/sample - loss: 0.0288 - val_loss: 0.1461\n",
      "Epoch 314/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.0285 - val_loss: 0.1488\n",
      "Epoch 315/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0303 - val_loss: 0.1356\n",
      "Epoch 316/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0271 - val_loss: 0.1590\n",
      "Epoch 317/600\n",
      "426/426 [==============================] - 0s 178us/sample - loss: 0.0283 - val_loss: 0.1467\n",
      "Epoch 318/600\n",
      "426/426 [==============================] - 0s 184us/sample - loss: 0.0287 - val_loss: 0.1496\n",
      "Epoch 319/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.0282 - val_loss: 0.1415\n",
      "Epoch 320/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0285 - val_loss: 0.1384\n",
      "Epoch 321/600\n",
      "426/426 [==============================] - 0s 175us/sample - loss: 0.0281 - val_loss: 0.1458\n",
      "Epoch 322/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0284 - val_loss: 0.1385\n",
      "Epoch 323/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0285 - val_loss: 0.1393\n",
      "Epoch 324/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0282 - val_loss: 0.1420\n",
      "Epoch 325/600\n",
      "426/426 [==============================] - 0s 177us/sample - loss: 0.0285 - val_loss: 0.1483\n",
      "Epoch 326/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0287 - val_loss: 0.1443\n",
      "Epoch 327/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0274 - val_loss: 0.1411\n",
      "Epoch 328/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0274 - val_loss: 0.1446\n",
      "Epoch 329/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.0272 - val_loss: 0.1415\n",
      "Epoch 330/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0284 - val_loss: 0.1493\n",
      "Epoch 331/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0278 - val_loss: 0.1459\n",
      "Epoch 332/600\n",
      "426/426 [==============================] - 0s 196us/sample - loss: 0.0285 - val_loss: 0.1433\n",
      "Epoch 333/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.0269 - val_loss: 0.1464\n",
      "Epoch 334/600\n",
      "426/426 [==============================] - 0s 215us/sample - loss: 0.0279 - val_loss: 0.1368\n",
      "Epoch 335/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0272 - val_loss: 0.1436\n",
      "Epoch 336/600\n",
      "426/426 [==============================] - 0s 147us/sample - loss: 0.0267 - val_loss: 0.1552\n",
      "Epoch 337/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0306 - val_loss: 0.1516\n",
      "Epoch 338/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0296 - val_loss: 0.1495\n",
      "Epoch 339/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0295 - val_loss: 0.1396\n",
      "Epoch 340/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0272 - val_loss: 0.1717\n",
      "Epoch 341/600\n",
      "426/426 [==============================] - 0s 190us/sample - loss: 0.0274 - val_loss: 0.1429\n",
      "Epoch 342/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.0279 - val_loss: 0.1701\n",
      "Epoch 343/600\n",
      "426/426 [==============================] - 0s 183us/sample - loss: 0.0300 - val_loss: 0.1424\n",
      "Epoch 344/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0276 - val_loss: 0.1506\n",
      "Epoch 345/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0279 - val_loss: 0.1490\n",
      "Epoch 346/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0261 - val_loss: 0.1467\n",
      "Epoch 347/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.0258 - val_loss: 0.1416\n",
      "Epoch 348/600\n",
      "426/426 [==============================] - 0s 307us/sample - loss: 0.0270 - val_loss: 0.1416\n",
      "Epoch 349/600\n",
      "426/426 [==============================] - 0s 143us/sample - loss: 0.0272 - val_loss: 0.1621\n",
      "Epoch 350/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0355 - val_loss: 0.1437\n",
      "Epoch 351/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0420 - val_loss: 0.1473\n",
      "Epoch 352/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0286 - val_loss: 0.1527\n",
      "Epoch 353/600\n",
      "426/426 [==============================] - 0s 262us/sample - loss: 0.0260 - val_loss: 0.1505\n",
      "Epoch 354/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0258 - val_loss: 0.1474\n",
      "Epoch 355/600\n",
      "426/426 [==============================] - 0s 295us/sample - loss: 0.0293 - val_loss: 0.1631\n",
      "Epoch 356/600\n",
      "426/426 [==============================] - 0s 298us/sample - loss: 0.0260 - val_loss: 0.1403\n",
      "Epoch 357/600\n",
      "426/426 [==============================] - 0s 273us/sample - loss: 0.0266 - val_loss: 0.1538\n",
      "Epoch 358/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0260 - val_loss: 0.1501\n",
      "Epoch 359/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0250 - val_loss: 0.1566\n",
      "Epoch 360/600\n",
      "426/426 [==============================] - 0s 189us/sample - loss: 0.0246 - val_loss: 0.1409\n",
      "Epoch 361/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0277 - val_loss: 0.1579\n",
      "Epoch 362/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0257 - val_loss: 0.1509\n",
      "Epoch 363/600\n",
      "426/426 [==============================] - 0s 194us/sample - loss: 0.0309 - val_loss: 0.1395\n",
      "Epoch 364/600\n",
      "426/426 [==============================] - 0s 178us/sample - loss: 0.0252 - val_loss: 0.1565\n",
      "Epoch 365/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0261 - val_loss: 0.1461\n",
      "Epoch 366/600\n",
      "426/426 [==============================] - 0s 190us/sample - loss: 0.0268 - val_loss: 0.1500\n",
      "Epoch 367/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0246 - val_loss: 0.1448\n",
      "Epoch 368/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0269 - val_loss: 0.1490\n",
      "Epoch 369/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0327 - val_loss: 0.1460\n",
      "Epoch 370/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0277 - val_loss: 0.1592\n",
      "Epoch 371/600\n",
      "426/426 [==============================] - 0s 215us/sample - loss: 0.0294 - val_loss: 0.1476\n",
      "Epoch 372/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.0244 - val_loss: 0.1533\n",
      "Epoch 373/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0247 - val_loss: 0.1446\n",
      "Epoch 374/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.0275 - val_loss: 0.1580\n",
      "Epoch 375/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0277 - val_loss: 0.1421\n",
      "Epoch 376/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0243 - val_loss: 0.1575\n",
      "Epoch 377/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0250 - val_loss: 0.1416\n",
      "Epoch 378/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0289 - val_loss: 0.1627\n",
      "Epoch 379/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0357 - val_loss: 0.1600\n",
      "Epoch 380/600\n",
      "426/426 [==============================] - 0s 207us/sample - loss: 0.0390 - val_loss: 0.1435\n",
      "Epoch 381/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0242 - val_loss: 0.1553\n",
      "Epoch 382/600\n",
      "426/426 [==============================] - 0s 153us/sample - loss: 0.0244 - val_loss: 0.1485\n",
      "Epoch 383/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0238 - val_loss: 0.1522\n",
      "Epoch 384/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0264 - val_loss: 0.1480\n",
      "Epoch 385/600\n",
      "426/426 [==============================] - 0s 261us/sample - loss: 0.0261 - val_loss: 0.1511\n",
      "Epoch 386/600\n",
      "426/426 [==============================] - 0s 153us/sample - loss: 0.0241 - val_loss: 0.1433\n",
      "Epoch 387/600\n",
      "426/426 [==============================] - 0s 186us/sample - loss: 0.0277 - val_loss: 0.1467\n",
      "Epoch 388/600\n",
      "426/426 [==============================] - 0s 180us/sample - loss: 0.0249 - val_loss: 0.1481\n",
      "Epoch 389/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0233 - val_loss: 0.1479\n",
      "Epoch 390/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0269 - val_loss: 0.1473\n",
      "Epoch 391/600\n",
      "426/426 [==============================] - 0s 200us/sample - loss: 0.0242 - val_loss: 0.1501\n",
      "Epoch 392/600\n",
      "426/426 [==============================] - 0s 271us/sample - loss: 0.0231 - val_loss: 0.1557\n",
      "Epoch 393/600\n",
      "426/426 [==============================] - 0s 266us/sample - loss: 0.0239 - val_loss: 0.1548\n",
      "Epoch 394/600\n",
      "426/426 [==============================] - 0s 259us/sample - loss: 0.0261 - val_loss: 0.1586\n",
      "Epoch 395/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0237 - val_loss: 0.1528\n",
      "Epoch 396/600\n",
      "426/426 [==============================] - 0s 344us/sample - loss: 0.0245 - val_loss: 0.1627\n",
      "Epoch 397/600\n",
      "426/426 [==============================] - 0s 226us/sample - loss: 0.0243 - val_loss: 0.1492\n",
      "Epoch 398/600\n",
      "426/426 [==============================] - 0s 178us/sample - loss: 0.0240 - val_loss: 0.1547\n",
      "Epoch 399/600\n",
      "426/426 [==============================] - 0s 234us/sample - loss: 0.0232 - val_loss: 0.1485\n",
      "Epoch 400/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.0228 - val_loss: 0.1481\n",
      "Epoch 401/600\n",
      "426/426 [==============================] - 0s 172us/sample - loss: 0.0252 - val_loss: 0.1534\n",
      "Epoch 402/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0232 - val_loss: 0.1503\n",
      "Epoch 403/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0240 - val_loss: 0.1506\n",
      "Epoch 404/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.0232 - val_loss: 0.1637\n",
      "Epoch 405/600\n",
      "426/426 [==============================] - 0s 250us/sample - loss: 0.0226 - val_loss: 0.1497\n",
      "Epoch 406/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.0236 - val_loss: 0.1643\n",
      "Epoch 407/600\n",
      "426/426 [==============================] - 0s 269us/sample - loss: 0.0289 - val_loss: 0.1477\n",
      "Epoch 408/600\n",
      "426/426 [==============================] - 0s 281us/sample - loss: 0.0248 - val_loss: 0.1570\n",
      "Epoch 409/600\n",
      "426/426 [==============================] - 0s 208us/sample - loss: 0.0225 - val_loss: 0.1518\n",
      "Epoch 410/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0281 - val_loss: 0.1619\n",
      "Epoch 411/600\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.024 - 0s 233us/sample - loss: 0.0237 - val_loss: 0.1490\n",
      "Epoch 412/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0229 - val_loss: 0.1514\n",
      "Epoch 413/600\n",
      "426/426 [==============================] - 0s 145us/sample - loss: 0.0304 - val_loss: 0.1453\n",
      "Epoch 414/600\n",
      "426/426 [==============================] - 0s 163us/sample - loss: 0.0233 - val_loss: 0.1488\n",
      "Epoch 415/600\n",
      "426/426 [==============================] - 0s 252us/sample - loss: 0.0240 - val_loss: 0.1508\n",
      "Epoch 416/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0227 - val_loss: 0.1552\n",
      "Epoch 417/600\n",
      "426/426 [==============================] - 0s 245us/sample - loss: 0.0222 - val_loss: 0.1545\n",
      "Epoch 418/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0216 - val_loss: 0.1529\n",
      "Epoch 419/600\n",
      "426/426 [==============================] - 0s 254us/sample - loss: 0.0221 - val_loss: 0.1479\n",
      "Epoch 420/600\n",
      "426/426 [==============================] - 0s 247us/sample - loss: 0.0219 - val_loss: 0.1611\n",
      "Epoch 421/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0215 - val_loss: 0.1501\n",
      "Epoch 422/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0213 - val_loss: 0.1533\n",
      "Epoch 423/600\n",
      "426/426 [==============================] - 0s 198us/sample - loss: 0.0216 - val_loss: 0.1557\n",
      "Epoch 424/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.0211 - val_loss: 0.1549\n",
      "Epoch 425/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0226 - val_loss: 0.1501\n",
      "Epoch 426/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0228 - val_loss: 0.1690\n",
      "Epoch 427/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0223 - val_loss: 0.1545\n",
      "Epoch 428/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0234 - val_loss: 0.1636\n",
      "Epoch 429/600\n",
      "426/426 [==============================] - 0s 280us/sample - loss: 0.0231 - val_loss: 0.1543\n",
      "Epoch 430/600\n",
      "426/426 [==============================] - 0s 262us/sample - loss: 0.0217 - val_loss: 0.1565\n",
      "Epoch 431/600\n",
      "426/426 [==============================] - 0s 248us/sample - loss: 0.0211 - val_loss: 0.1661\n",
      "Epoch 432/600\n",
      "426/426 [==============================] - 0s 292us/sample - loss: 0.0244 - val_loss: 0.1671\n",
      "Epoch 433/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0209 - val_loss: 0.1569\n",
      "Epoch 434/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0211 - val_loss: 0.1592\n",
      "Epoch 435/600\n",
      "426/426 [==============================] - 0s 184us/sample - loss: 0.0203 - val_loss: 0.1551\n",
      "Epoch 436/600\n",
      "426/426 [==============================] - 0s 180us/sample - loss: 0.0209 - val_loss: 0.1507\n",
      "Epoch 437/600\n",
      "426/426 [==============================] - 0s 147us/sample - loss: 0.0205 - val_loss: 0.1588\n",
      "Epoch 438/600\n",
      "426/426 [==============================] - 0s 144us/sample - loss: 0.0199 - val_loss: 0.1539\n",
      "Epoch 439/600\n",
      "426/426 [==============================] - 0s 187us/sample - loss: 0.0218 - val_loss: 0.1517\n",
      "Epoch 440/600\n",
      "426/426 [==============================] - 0s 193us/sample - loss: 0.0214 - val_loss: 0.1594\n",
      "Epoch 441/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0210 - val_loss: 0.1499\n",
      "Epoch 442/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0236 - val_loss: 0.1662\n",
      "Epoch 443/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0195 - val_loss: 0.1498\n",
      "Epoch 444/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0208 - val_loss: 0.1598\n",
      "Epoch 445/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0200 - val_loss: 0.1575\n",
      "Epoch 446/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.0196 - val_loss: 0.1601\n",
      "Epoch 447/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0199 - val_loss: 0.1541\n",
      "Epoch 448/600\n",
      "426/426 [==============================] - 0s 262us/sample - loss: 0.0195 - val_loss: 0.1699\n",
      "Epoch 449/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.0225 - val_loss: 0.1579\n",
      "Epoch 450/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0216 - val_loss: 0.1550\n",
      "Epoch 451/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0314 - val_loss: 0.2054\n",
      "Epoch 452/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0364 - val_loss: 0.1555\n",
      "Epoch 453/600\n",
      "426/426 [==============================] - 0s 288us/sample - loss: 0.0310 - val_loss: 0.1772\n",
      "Epoch 454/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0225 - val_loss: 0.1647\n",
      "Epoch 455/600\n",
      "426/426 [==============================] - 0s 255us/sample - loss: 0.0203 - val_loss: 0.1762\n",
      "Epoch 456/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.0192 - val_loss: 0.1657\n",
      "Epoch 457/600\n",
      "426/426 [==============================] - 0s 234us/sample - loss: 0.0200 - val_loss: 0.1829\n",
      "Epoch 458/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0197 - val_loss: 0.1736\n",
      "Epoch 459/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 234us/sample - loss: 0.0188 - val_loss: 0.1721\n",
      "Epoch 460/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0196 - val_loss: 0.1737\n",
      "Epoch 461/600\n",
      "426/426 [==============================] - 0s 222us/sample - loss: 0.0178 - val_loss: 0.1762\n",
      "Epoch 462/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0185 - val_loss: 0.1737\n",
      "Epoch 463/600\n",
      "426/426 [==============================] - 0s 177us/sample - loss: 0.0226 - val_loss: 0.1665\n",
      "Epoch 464/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0206 - val_loss: 0.1841\n",
      "Epoch 465/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0181 - val_loss: 0.1713\n",
      "Epoch 466/600\n",
      "426/426 [==============================] - 0s 186us/sample - loss: 0.0184 - val_loss: 0.1799\n",
      "Epoch 467/600\n",
      "426/426 [==============================] - 0s 198us/sample - loss: 0.0178 - val_loss: 0.1719\n",
      "Epoch 468/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.0168 - val_loss: 0.1809\n",
      "Epoch 469/600\n",
      "426/426 [==============================] - 0s 183us/sample - loss: 0.0172 - val_loss: 0.1727\n",
      "Epoch 470/600\n",
      "426/426 [==============================] - 0s 170us/sample - loss: 0.0171 - val_loss: 0.1730\n",
      "Epoch 471/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0185 - val_loss: 0.1858\n",
      "Epoch 472/600\n",
      "426/426 [==============================] - 0s 254us/sample - loss: 0.0190 - val_loss: 0.1729\n",
      "Epoch 473/600\n",
      "426/426 [==============================] - 0s 148us/sample - loss: 0.0234 - val_loss: 0.1855\n",
      "Epoch 474/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0256 - val_loss: 0.1648\n",
      "Epoch 475/600\n",
      "426/426 [==============================] - 0s 194us/sample - loss: 0.0263 - val_loss: 0.1929\n",
      "Epoch 476/600\n",
      "426/426 [==============================] - 0s 170us/sample - loss: 0.0168 - val_loss: 0.1696\n",
      "Epoch 477/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0181 - val_loss: 0.1931\n",
      "Epoch 478/600\n",
      "426/426 [==============================] - 0s 177us/sample - loss: 0.0166 - val_loss: 0.1762\n",
      "Epoch 479/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0167 - val_loss: 0.1757\n",
      "Epoch 480/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0165 - val_loss: 0.1808\n",
      "Epoch 481/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0166 - val_loss: 0.1748\n",
      "Epoch 482/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.0188 - val_loss: 0.1722\n",
      "Epoch 483/600\n",
      "426/426 [==============================] - 0s 307us/sample - loss: 0.0222 - val_loss: 0.1781\n",
      "Epoch 484/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0216 - val_loss: 0.1821\n",
      "Epoch 485/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0249 - val_loss: 0.1684\n",
      "Epoch 486/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0200 - val_loss: 0.1701\n",
      "Epoch 487/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.0191 - val_loss: 0.1757\n",
      "Epoch 488/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0163 - val_loss: 0.1701\n",
      "Epoch 489/600\n",
      "426/426 [==============================] - 0s 163us/sample - loss: 0.0159 - val_loss: 0.1740\n",
      "Epoch 490/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.0154 - val_loss: 0.1745\n",
      "Epoch 491/600\n",
      "426/426 [==============================] - 0s 209us/sample - loss: 0.0156 - val_loss: 0.1749\n",
      "Epoch 492/600\n",
      "426/426 [==============================] - 0s 261us/sample - loss: 0.0173 - val_loss: 0.1729\n",
      "Epoch 493/600\n",
      "426/426 [==============================] - 0s 271us/sample - loss: 0.0162 - val_loss: 0.1733\n",
      "Epoch 494/600\n",
      "426/426 [==============================] - 0s 263us/sample - loss: 0.0154 - val_loss: 0.1785\n",
      "Epoch 495/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0160 - val_loss: 0.1711\n",
      "Epoch 496/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0174 - val_loss: 0.1861\n",
      "Epoch 497/600\n",
      "426/426 [==============================] - 0s 208us/sample - loss: 0.0175 - val_loss: 0.1722\n",
      "Epoch 498/600\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.012 - 0s 292us/sample - loss: 0.0142 - val_loss: 0.1879\n",
      "Epoch 499/600\n",
      "426/426 [==============================] - 0s 220us/sample - loss: 0.0160 - val_loss: 0.1692\n",
      "Epoch 500/600\n",
      "426/426 [==============================] - 0s 206us/sample - loss: 0.0157 - val_loss: 0.1728\n",
      "Epoch 501/600\n",
      "426/426 [==============================] - 0s 190us/sample - loss: 0.0167 - val_loss: 0.1719\n",
      "Epoch 502/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0158 - val_loss: 0.1673\n",
      "Epoch 503/600\n",
      "426/426 [==============================] - 0s 144us/sample - loss: 0.0149 - val_loss: 0.1807\n",
      "Epoch 504/600\n",
      "426/426 [==============================] - 0s 172us/sample - loss: 0.0148 - val_loss: 0.1780\n",
      "Epoch 505/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0159 - val_loss: 0.1793\n",
      "Epoch 506/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0144 - val_loss: 0.1788\n",
      "Epoch 507/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0139 - val_loss: 0.1799\n",
      "Epoch 508/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0168 - val_loss: 0.1868\n",
      "Epoch 509/600\n",
      "426/426 [==============================] - 0s 170us/sample - loss: 0.0151 - val_loss: 0.1792\n",
      "Epoch 510/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.0143 - val_loss: 0.1785\n",
      "Epoch 511/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0137 - val_loss: 0.1745\n",
      "Epoch 512/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.0145 - val_loss: 0.1838\n",
      "Epoch 513/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0154 - val_loss: 0.1891\n",
      "Epoch 514/600\n",
      "426/426 [==============================] - 0s 236us/sample - loss: 0.0158 - val_loss: 0.1715\n",
      "Epoch 515/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.0150 - val_loss: 0.1764\n",
      "Epoch 516/600\n",
      "426/426 [==============================] - 0s 178us/sample - loss: 0.0146 - val_loss: 0.1823\n",
      "Epoch 517/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0132 - val_loss: 0.1814\n",
      "Epoch 518/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0137 - val_loss: 0.1738\n",
      "Epoch 519/600\n",
      "426/426 [==============================] - 0s 261us/sample - loss: 0.0136 - val_loss: 0.1842\n",
      "Epoch 520/600\n",
      "426/426 [==============================] - 0s 255us/sample - loss: 0.0132 - val_loss: 0.1767\n",
      "Epoch 521/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.0143 - val_loss: 0.1796\n",
      "Epoch 522/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0139 - val_loss: 0.1726\n",
      "Epoch 523/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0138 - val_loss: 0.1807\n",
      "Epoch 524/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.0148 - val_loss: 0.1817\n",
      "Epoch 525/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0134 - val_loss: 0.1813\n",
      "Epoch 526/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0121 - val_loss: 0.1736\n",
      "Epoch 527/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0129 - val_loss: 0.1851\n",
      "Epoch 528/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0130 - val_loss: 0.1738\n",
      "Epoch 529/600\n",
      "426/426 [==============================] - 0s 203us/sample - loss: 0.0131 - val_loss: 0.1844\n",
      "Epoch 530/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.0133 - val_loss: 0.1782\n",
      "Epoch 531/600\n",
      "426/426 [==============================] - 0s 254us/sample - loss: 0.0128 - val_loss: 0.1796\n",
      "Epoch 532/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0122 - val_loss: 0.1793\n",
      "Epoch 533/600\n",
      "426/426 [==============================] - 0s 189us/sample - loss: 0.0124 - val_loss: 0.1774\n",
      "Epoch 534/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0128 - val_loss: 0.1830\n",
      "Epoch 535/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0121 - val_loss: 0.1907\n",
      "Epoch 536/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.0120 - val_loss: 0.1860\n",
      "Epoch 537/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0121 - val_loss: 0.1925\n",
      "Epoch 538/600\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.002 - 0s 205us/sample - loss: 0.0124 - val_loss: 0.1888\n",
      "Epoch 539/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0131 - val_loss: 0.1801\n",
      "Epoch 540/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0119 - val_loss: 0.1873\n",
      "Epoch 541/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0113 - val_loss: 0.1879\n",
      "Epoch 542/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.0118 - val_loss: 0.1895\n",
      "Epoch 543/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0114 - val_loss: 0.1817\n",
      "Epoch 544/600\n",
      "426/426 [==============================] - 0s 277us/sample - loss: 0.0112 - val_loss: 0.1893\n",
      "Epoch 545/600\n",
      "426/426 [==============================] - 0s 268us/sample - loss: 0.0125 - val_loss: 0.1937\n",
      "Epoch 546/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0123 - val_loss: 0.1836\n",
      "Epoch 547/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.0111 - val_loss: 0.2033\n",
      "Epoch 548/600\n",
      "426/426 [==============================] - 0s 183us/sample - loss: 0.0113 - val_loss: 0.1816\n",
      "Epoch 549/600\n",
      "426/426 [==============================] - 0s 178us/sample - loss: 0.0111 - val_loss: 0.1935\n",
      "Epoch 550/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0118 - val_loss: 0.1793\n",
      "Epoch 551/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0114 - val_loss: 0.1879\n",
      "Epoch 552/600\n",
      "426/426 [==============================] - 0s 295us/sample - loss: 0.0110 - val_loss: 0.1919\n",
      "Epoch 553/600\n",
      "426/426 [==============================] - 0s 236us/sample - loss: 0.0115 - val_loss: 0.1809\n",
      "Epoch 554/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.0116 - val_loss: 0.1909\n",
      "Epoch 555/600\n",
      "426/426 [==============================] - 0s 184us/sample - loss: 0.0108 - val_loss: 0.1817\n",
      "Epoch 556/600\n",
      "426/426 [==============================] - 0s 234us/sample - loss: 0.0107 - val_loss: 0.1922\n",
      "Epoch 557/600\n",
      "426/426 [==============================] - 0s 206us/sample - loss: 0.0116 - val_loss: 0.1929\n",
      "Epoch 558/600\n",
      "426/426 [==============================] - 0s 229us/sample - loss: 0.0115 - val_loss: 0.1996\n",
      "Epoch 559/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0106 - val_loss: 0.1911\n",
      "Epoch 560/600\n",
      "426/426 [==============================] - 0s 243us/sample - loss: 0.0109 - val_loss: 0.2016\n",
      "Epoch 561/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0106 - val_loss: 0.1950\n",
      "Epoch 562/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0106 - val_loss: 0.1977\n",
      "Epoch 563/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0108 - val_loss: 0.1986\n",
      "Epoch 564/600\n",
      "426/426 [==============================] - 0s 180us/sample - loss: 0.0115 - val_loss: 0.1895\n",
      "Epoch 565/600\n",
      "426/426 [==============================] - 0s 181us/sample - loss: 0.0126 - val_loss: 0.1853\n",
      "Epoch 566/600\n",
      "426/426 [==============================] - 0s 144us/sample - loss: 0.0113 - val_loss: 0.1872\n",
      "Epoch 567/600\n",
      "426/426 [==============================] - 0s 190us/sample - loss: 0.0105 - val_loss: 0.1957\n",
      "Epoch 568/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0100 - val_loss: 0.1893\n",
      "Epoch 569/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0107 - val_loss: 0.1975\n",
      "Epoch 570/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0112 - val_loss: 0.1853\n",
      "Epoch 571/600\n",
      "426/426 [==============================] - 0s 187us/sample - loss: 0.0105 - val_loss: 0.2009\n",
      "Epoch 572/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0099 - val_loss: 0.1865\n",
      "Epoch 573/600\n",
      "426/426 [==============================] - 0s 163us/sample - loss: 0.0108 - val_loss: 0.1936\n",
      "Epoch 574/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0110 - val_loss: 0.1894\n",
      "Epoch 575/600\n",
      "426/426 [==============================] - 0s 147us/sample - loss: 0.0096 - val_loss: 0.1986\n",
      "Epoch 576/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.0094 - val_loss: 0.2000\n",
      "Epoch 577/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.0094 - val_loss: 0.1976\n",
      "Epoch 578/600\n",
      "426/426 [==============================] - 0s 335us/sample - loss: 0.0104 - val_loss: 0.2023\n",
      "Epoch 579/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0110 - val_loss: 0.1973\n",
      "Epoch 580/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0100 - val_loss: 0.1940\n",
      "Epoch 581/600\n",
      "426/426 [==============================] - 0s 177us/sample - loss: 0.0090 - val_loss: 0.1937\n",
      "Epoch 582/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0088 - val_loss: 0.1992\n",
      "Epoch 583/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0088 - val_loss: 0.1974\n",
      "Epoch 584/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0087 - val_loss: 0.2014\n",
      "Epoch 585/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0087 - val_loss: 0.1958\n",
      "Epoch 586/600\n",
      "426/426 [==============================] - 0s 245us/sample - loss: 0.0089 - val_loss: 0.1975\n",
      "Epoch 587/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0086 - val_loss: 0.1927\n",
      "Epoch 588/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0088 - val_loss: 0.1996\n",
      "Epoch 589/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0090 - val_loss: 0.1997\n",
      "Epoch 590/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.0094 - val_loss: 0.1964\n",
      "Epoch 591/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.0085 - val_loss: 0.1954\n",
      "Epoch 592/600\n",
      "426/426 [==============================] - 0s 181us/sample - loss: 0.0086 - val_loss: 0.1943\n",
      "Epoch 593/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.0083 - val_loss: 0.1971\n",
      "Epoch 594/600\n",
      "426/426 [==============================] - 0s 150us/sample - loss: 0.0082 - val_loss: 0.1965\n",
      "Epoch 595/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.0084 - val_loss: 0.1971\n",
      "Epoch 596/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0081 - val_loss: 0.2052\n",
      "Epoch 597/600\n",
      "426/426 [==============================] - 0s 183us/sample - loss: 0.0085 - val_loss: 0.1925\n",
      "Epoch 598/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.0103 - val_loss: 0.2132\n",
      "Epoch 599/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0079 - val_loss: 0.1931\n",
      "Epoch 600/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0083 - val_loss: 0.2056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e4cd27a108>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.fit(x=x_train,y=y_train,epochs=600,validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e4cec83808>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Model_Loss=pd.DataFrame(Model.history.history)\n",
    "Model_Loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682883</td>\n",
       "      <td>0.649232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.626934</td>\n",
       "      <td>0.604978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.581133</td>\n",
       "      <td>0.561827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.537046</td>\n",
       "      <td>0.516445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493616</td>\n",
       "      <td>0.471596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.205249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.008467</td>\n",
       "      <td>0.192454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.213193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.193084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.008346</td>\n",
       "      <td>0.205564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  val_loss\n",
       "0    0.682883  0.649232\n",
       "1    0.626934  0.604978\n",
       "2    0.581133  0.561827\n",
       "3    0.537046  0.516445\n",
       "4    0.493616  0.471596\n",
       "..        ...       ...\n",
       "595  0.008054  0.205249\n",
       "596  0.008467  0.192454\n",
       "597  0.010262  0.213193\n",
       "598  0.007881  0.193084\n",
       "599  0.008346  0.205564\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_Loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EARLY STOPPING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1 = Sequential()\n",
    "Model1.add(Dense(units=30,activation='relu',kernel_initializer='he_uniform'))\n",
    "Model1.add(Dense(units=15,activation='relu',kernel_initializer='he_uniform'))\n",
    "\n",
    "Model1.add(Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform'))\n",
    "\n",
    "Model1.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "class EarlyStopping(Callback)\n",
      " |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |  \n",
      " |  Stop training when a monitored quantity has stopped improving.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      monitor: Quantity to be monitored.\n",
      " |      min_delta: Minimum change in the monitored quantity\n",
      " |          to qualify as an improvement, i.e. an absolute\n",
      " |          change of less than min_delta, will count as no\n",
      " |          improvement.\n",
      " |      patience: Number of epochs with no improvement\n",
      " |          after which training will be stopped.\n",
      " |      verbose: verbosity mode.\n",
      " |      mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
      " |          training will stop when the quantity\n",
      " |          monitored has stopped decreasing; in `max`\n",
      " |          mode it will stop when the quantity\n",
      " |          monitored has stopped increasing; in `auto`\n",
      " |          mode, the direction is automatically inferred\n",
      " |          from the name of the monitored quantity.\n",
      " |      baseline: Baseline value for the monitored quantity.\n",
      " |          Training will stop if the model doesn't show improvement over the\n",
      " |          baseline.\n",
      " |      restore_best_weights: Whether to restore model weights from\n",
      " |          the epoch with the best value of the monitored quantity.\n",
      " |          If False, the model weights obtained at the last step of\n",
      " |          training are used.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
      " |  # This callback will stop the training when there is no improvement in\n",
      " |  # the validation loss for three consecutive epochs.\n",
      " |  model.fit(data, labels, epochs=100, callbacks=[callback],\n",
      " |      validation_data=(val_data, val_labels))\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_monitor_value(self, logs)\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: integer, index of epoch.\n",
      " |          logs: dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: integer, index of epoch.\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
      " |            number and the size of the batch.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Metric results for this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
      " |            number and the size of the batch.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Metric results for this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
      " |            number and the size of the batch.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Metric results for this batch.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EARLY STOPPING THE TECHNIQUE TO PREVENT OVERFITTING (STOP TRAINING EARLY BEFORE WE OVERFIT) \n",
    "i.MINIMIZE LOSS \n",
    "ii.MAXIMIZE ACCURACY \n",
    "iii.MONITOR='VAL_LOSS'(WE WANT TO TRACK VALIDATION LOSS WHICH WE WANT TO MINIMIZE),IF LOSS=0 (WE HAVE A PERFECT FIT) I.E MINIMUM FOR VALIDATION LOSS,MEAN SQUARED ERROR OR MAXIMUM FOR ACCURACY SCORE \n",
    "PATIENCE=25 (WE WILL WAIT FOR MORE 25 EPOCHS EVEN AFTER WE DECTECTED THERE IS NO IMPROVEMENT OR STOPPING POINT BECAUSE OF NOISE COULD OCCUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Early_Stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 1s 3ms/sample - loss: 0.6697 - val_loss: 0.6454\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.6313 - val_loss: 0.6106\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.5933 - val_loss: 0.5738\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 186us/sample - loss: 0.5527 - val_loss: 0.5310\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 163us/sample - loss: 0.5101 - val_loss: 0.4872\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.4642 - val_loss: 0.4407\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.4171 - val_loss: 0.3891\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.3704 - val_loss: 0.3448\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.3316 - val_loss: 0.3079\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.3005 - val_loss: 0.2791\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.2740 - val_loss: 0.2583\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.2550 - val_loss: 0.2380\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 182us/sample - loss: 0.2360 - val_loss: 0.2259\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.2202 - val_loss: 0.2088\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 226us/sample - loss: 0.2080 - val_loss: 0.1995\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 231us/sample - loss: 0.1971 - val_loss: 0.1900\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 283us/sample - loss: 0.1854 - val_loss: 0.1824\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 292us/sample - loss: 0.1763 - val_loss: 0.1720\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 298us/sample - loss: 0.1676 - val_loss: 0.1654\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.1594 - val_loss: 0.1612\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 270us/sample - loss: 0.1537 - val_loss: 0.1555\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.1476 - val_loss: 0.1498\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.1435 - val_loss: 0.1428\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.1414 - val_loss: 0.1419\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.1295 - val_loss: 0.1338\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.1239 - val_loss: 0.1344\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 295us/sample - loss: 0.1199 - val_loss: 0.1295\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 205us/sample - loss: 0.1147 - val_loss: 0.1258\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.1154 - val_loss: 0.1239\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.1066 - val_loss: 0.1220\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 184us/sample - loss: 0.1046 - val_loss: 0.1200\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.1007 - val_loss: 0.1171\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 180us/sample - loss: 0.0969 - val_loss: 0.1134\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0952 - val_loss: 0.1139\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0912 - val_loss: 0.1117\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0881 - val_loss: 0.1109\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 153us/sample - loss: 0.0859 - val_loss: 0.1109\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0857 - val_loss: 0.1095\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0830 - val_loss: 0.1085\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0799 - val_loss: 0.1048\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0783 - val_loss: 0.1094\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0785 - val_loss: 0.1051\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 215us/sample - loss: 0.0773 - val_loss: 0.1058\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0743 - val_loss: 0.1075\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 178us/sample - loss: 0.0750 - val_loss: 0.1021\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 359us/sample - loss: 0.0723 - val_loss: 0.1049\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 193us/sample - loss: 0.0712 - val_loss: 0.1029\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0722 - val_loss: 0.0991\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 222us/sample - loss: 0.0691 - val_loss: 0.1010\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0703 - val_loss: 0.0990\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0666 - val_loss: 0.0983\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 207us/sample - loss: 0.0664 - val_loss: 0.0981\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 217us/sample - loss: 0.0651 - val_loss: 0.1006\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 175us/sample - loss: 0.0647 - val_loss: 0.0975\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 175us/sample - loss: 0.0637 - val_loss: 0.0969\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 177us/sample - loss: 0.0628 - val_loss: 0.0968\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0632 - val_loss: 0.1004\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 178us/sample - loss: 0.0633 - val_loss: 0.0971\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 307us/sample - loss: 0.0613 - val_loss: 0.0995\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 209us/sample - loss: 0.0601 - val_loss: 0.0973\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0604 - val_loss: 0.1027\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0594 - val_loss: 0.0975\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0589 - val_loss: 0.0984\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 170us/sample - loss: 0.0583 - val_loss: 0.0949\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0575 - val_loss: 0.0936\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0599 - val_loss: 0.1005\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 269us/sample - loss: 0.0572 - val_loss: 0.1027\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.0579 - val_loss: 0.1011\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 147us/sample - loss: 0.0578 - val_loss: 0.0919\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 163us/sample - loss: 0.0569 - val_loss: 0.0988\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0561 - val_loss: 0.1066\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0559 - val_loss: 0.0931\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 188us/sample - loss: 0.0563 - val_loss: 0.1040\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0552 - val_loss: 0.0981\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0546 - val_loss: 0.0922\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 250us/sample - loss: 0.0523 - val_loss: 0.1078\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0557 - val_loss: 0.0933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.0555 - val_loss: 0.1060\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0548 - val_loss: 0.1032\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 209us/sample - loss: 0.0521 - val_loss: 0.0970\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.0520 - val_loss: 0.0978\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0523 - val_loss: 0.0962\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 371us/sample - loss: 0.0514 - val_loss: 0.1008\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0511 - val_loss: 0.0933\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0549 - val_loss: 0.1003\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.0511 - val_loss: 0.0947\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 181us/sample - loss: 0.0510 - val_loss: 0.1038\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0526 - val_loss: 0.0986\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 188us/sample - loss: 0.0497 - val_loss: 0.1048\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0518 - val_loss: 0.1011\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 186us/sample - loss: 0.0500 - val_loss: 0.0977\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0503 - val_loss: 0.1024\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 207us/sample - loss: 0.0492 - val_loss: 0.0965\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 279us/sample - loss: 0.0484 - val_loss: 0.1013\n",
      "Epoch 00094: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e4cf5e2dc8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1.fit(x=x_train,y=y_train,epochs=600,validation_data=(x_test, y_test), verbose=1,callbacks=[Early_Stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e4d1bca088>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Model1_Loss=pd.DataFrame(Model1.history.history)\n",
    "Model1_Loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model2 = Sequential()\n",
    "Model2.add(Dense(units=30,activation='relu',kernel_initializer='he_uniform'))\n",
    "Model2.add(Dropout(0.5))\n",
    "\n",
    "Model2.add(Dense(units=15,activation='relu',kernel_initializer='he_uniform'))\n",
    "Model2.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "Model2.add(Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform'))\n",
    "\n",
    "Model2.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 1s 1ms/sample - loss: 0.6910 - val_loss: 0.6398\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.6855 - val_loss: 0.6187\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.6799 - val_loss: 0.6009\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.6496 - val_loss: 0.5727\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.6316 - val_loss: 0.5487\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.6119 - val_loss: 0.5309\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 79us/sample - loss: 0.5701 - val_loss: 0.5077\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.5669 - val_loss: 0.4807\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.5429 - val_loss: 0.4577\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.5300 - val_loss: 0.4355\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 83us/sample - loss: 0.5080 - val_loss: 0.4119\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.4784 - val_loss: 0.3905\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.4538 - val_loss: 0.3644\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.4546 - val_loss: 0.3403\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.4108 - val_loss: 0.3121\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.4109 - val_loss: 0.2930\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 92us/sample - loss: 0.3908 - val_loss: 0.2788\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.4025 - val_loss: 0.2690\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.3700 - val_loss: 0.2592\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.3451 - val_loss: 0.2426\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.3525 - val_loss: 0.2292\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.3611 - val_loss: 0.2203\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 78us/sample - loss: 0.3385 - val_loss: 0.2146\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.3088 - val_loss: 0.2049\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.3139 - val_loss: 0.1973\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.3213 - val_loss: 0.1932\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 79us/sample - loss: 0.3100 - val_loss: 0.1918\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.3031 - val_loss: 0.1856\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.2513 - val_loss: 0.1797\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.2833 - val_loss: 0.1664\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.2738 - val_loss: 0.1630\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.2867 - val_loss: 0.1572\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.2620 - val_loss: 0.1516\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.2540 - val_loss: 0.1480\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.2699 - val_loss: 0.1425\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.2513 - val_loss: 0.1373\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.2375 - val_loss: 0.1293\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 74us/sample - loss: 0.2655 - val_loss: 0.1285\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 79us/sample - loss: 0.2315 - val_loss: 0.1308\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 81us/sample - loss: 0.2460 - val_loss: 0.1280\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 89us/sample - loss: 0.2126 - val_loss: 0.1285\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.2118 - val_loss: 0.1245\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.2035 - val_loss: 0.1233\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.1955 - val_loss: 0.1184\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.2171 - val_loss: 0.1122\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 101us/sample - loss: 0.2138 - val_loss: 0.1109\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 89us/sample - loss: 0.1928 - val_loss: 0.1127\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.1863 - val_loss: 0.1118\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 79us/sample - loss: 0.1677 - val_loss: 0.1086\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.2643 - val_loss: 0.1105\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.1916 - val_loss: 0.1079\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.1925 - val_loss: 0.1059\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 131us/sample - loss: 0.1914 - val_loss: 0.1052\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.1822 - val_loss: 0.1028\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.1804 - val_loss: 0.1054\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 91us/sample - loss: 0.1589 - val_loss: 0.0994\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 73us/sample - loss: 0.1628 - val_loss: 0.1059\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.1801 - val_loss: 0.0993\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.1777 - val_loss: 0.0957\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 89us/sample - loss: 0.1587 - val_loss: 0.1043\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.1546 - val_loss: 0.1077\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.1732 - val_loss: 0.0947\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1600 - val_loss: 0.1044\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.1628 - val_loss: 0.0972\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 73us/sample - loss: 0.1731 - val_loss: 0.0953\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.1656 - val_loss: 0.0925\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 98us/sample - loss: 0.1706 - val_loss: 0.0906\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 108us/sample - loss: 0.1473 - val_loss: 0.0912\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1419 - val_loss: 0.0921\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.1514 - val_loss: 0.0891\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1563 - val_loss: 0.0924\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 70us/sample - loss: 0.1655 - val_loss: 0.0899\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 70us/sample - loss: 0.1529 - val_loss: 0.0887\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 79us/sample - loss: 0.1190 - val_loss: 0.0873\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 91us/sample - loss: 0.1719 - val_loss: 0.0854\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1347 - val_loss: 0.0859\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1519 - val_loss: 0.0912\n",
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1268 - val_loss: 0.0908\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 96us/sample - loss: 0.1436 - val_loss: 0.0856\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.1665 - val_loss: 0.0891\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.1583 - val_loss: 0.0901\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.1378 - val_loss: 0.0847\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1556 - val_loss: 0.0903\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 82us/sample - loss: 0.1280 - val_loss: 0.0818\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 66us/sample - loss: 0.1347 - val_loss: 0.0850\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 72us/sample - loss: 0.1259 - val_loss: 0.0836\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 87us/sample - loss: 0.1332 - val_loss: 0.0835\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.1193 - val_loss: 0.0847\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.1223 - val_loss: 0.0830\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1243 - val_loss: 0.0923\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.0974 - val_loss: 0.0871\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1280 - val_loss: 0.0798\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.1257 - val_loss: 0.0894\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.1050 - val_loss: 0.0829\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.1112 - val_loss: 0.0807\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1166 - val_loss: 0.0858\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1165 - val_loss: 0.0864\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1121 - val_loss: 0.0839\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1141 - val_loss: 0.0804\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 72us/sample - loss: 0.1600 - val_loss: 0.0834\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1082 - val_loss: 0.0875\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 73us/sample - loss: 0.1105 - val_loss: 0.0902\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 73us/sample - loss: 0.1214 - val_loss: 0.0824\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 73us/sample - loss: 0.1045 - val_loss: 0.0809\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 101us/sample - loss: 0.1224 - val_loss: 0.0802\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.1126 - val_loss: 0.0799\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 91us/sample - loss: 0.1003 - val_loss: 0.0815\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.183 - 0s 84us/sample - loss: 0.0981 - val_loss: 0.0945\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 73us/sample - loss: 0.1116 - val_loss: 0.0937\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.1050 - val_loss: 0.0829\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 80us/sample - loss: 0.1102 - val_loss: 0.0820\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 77us/sample - loss: 0.1111 - val_loss: 0.0823\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 73us/sample - loss: 0.1024 - val_loss: 0.0824\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.0996 - val_loss: 0.0824\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.0996 - val_loss: 0.0805\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1233 - val_loss: 0.0836\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 75us/sample - loss: 0.1249 - val_loss: 0.0836\n",
      "Epoch 00117: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e4d1e49bc8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model2.fit(x=x_train,y=y_train,epochs=600,validation_data=(x_test, y_test), verbose=1,callbacks=[Early_Stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e4d35108c8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Model2_Loss=pd.DataFrame(Model2.history.history)\n",
    "Model2_Loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.88915563e-01],\n",
       "       [9.88156438e-01],\n",
       "       [9.90978122e-01],\n",
       "       [1.52862072e-02],\n",
       "       [9.99065340e-01],\n",
       "       [9.98941898e-01],\n",
       "       [9.98575687e-01],\n",
       "       [1.01327896e-06],\n",
       "       [9.94238257e-01],\n",
       "       [9.96621370e-01],\n",
       "       [2.00471282e-03],\n",
       "       [9.93607044e-01],\n",
       "       [9.47030783e-01],\n",
       "       [9.97967124e-01],\n",
       "       [1.45062804e-03],\n",
       "       [9.98094440e-01],\n",
       "       [9.87957776e-01],\n",
       "       [9.98777986e-01],\n",
       "       [7.83801079e-06],\n",
       "       [9.72299576e-02],\n",
       "       [9.63543415e-01],\n",
       "       [9.96601343e-01],\n",
       "       [9.77234721e-01],\n",
       "       [9.98123050e-01],\n",
       "       [1.00669265e-03],\n",
       "       [9.66683745e-01],\n",
       "       [8.35329294e-04],\n",
       "       [9.99843061e-01],\n",
       "       [5.89638948e-04],\n",
       "       [1.04387999e-02],\n",
       "       [3.24205130e-01],\n",
       "       [5.01894951e-03],\n",
       "       [7.32450187e-01],\n",
       "       [2.19606549e-01],\n",
       "       [9.98451889e-01],\n",
       "       [9.99059260e-01],\n",
       "       [1.42661035e-02],\n",
       "       [4.34393883e-02],\n",
       "       [1.43617392e-04],\n",
       "       [3.93390656e-06],\n",
       "       [2.90569663e-03],\n",
       "       [9.92049575e-01],\n",
       "       [9.98103738e-01],\n",
       "       [9.30322051e-01],\n",
       "       [9.99660134e-01],\n",
       "       [9.95890617e-01],\n",
       "       [1.53481960e-04],\n",
       "       [7.78162956e-01],\n",
       "       [4.11060154e-02],\n",
       "       [9.97374415e-01],\n",
       "       [8.18490982e-04],\n",
       "       [9.12665009e-01],\n",
       "       [9.85789776e-01],\n",
       "       [8.94069672e-08],\n",
       "       [9.98245478e-01],\n",
       "       [9.97545302e-01],\n",
       "       [8.36849213e-05],\n",
       "       [2.93254852e-05],\n",
       "       [9.92473364e-01],\n",
       "       [9.97243285e-01],\n",
       "       [1.57380104e-03],\n",
       "       [3.71175587e-01],\n",
       "       [9.73410726e-01],\n",
       "       [9.97692108e-01],\n",
       "       [7.00354576e-06],\n",
       "       [8.72750044e-01],\n",
       "       [9.30815578e-01],\n",
       "       [5.36441803e-07],\n",
       "       [8.82118940e-04],\n",
       "       [9.87654924e-01],\n",
       "       [1.40607357e-04],\n",
       "       [9.98665690e-01],\n",
       "       [9.99337316e-01],\n",
       "       [9.96845126e-01],\n",
       "       [3.88532877e-04],\n",
       "       [1.96695328e-06],\n",
       "       [9.87119496e-01],\n",
       "       [9.96031046e-01],\n",
       "       [1.73151493e-05],\n",
       "       [3.25774848e-01],\n",
       "       [8.89644384e-01],\n",
       "       [9.72476840e-01],\n",
       "       [9.99595582e-01],\n",
       "       [9.94327128e-01],\n",
       "       [9.88496900e-01],\n",
       "       [9.99948323e-01],\n",
       "       [1.99385285e-02],\n",
       "       [9.88361359e-01],\n",
       "       [2.19374895e-04],\n",
       "       [2.71662116e-01],\n",
       "       [9.92870927e-01],\n",
       "       [4.73203361e-02],\n",
       "       [7.65261054e-03],\n",
       "       [8.87838840e-01],\n",
       "       [9.99168038e-01],\n",
       "       [9.98975575e-01],\n",
       "       [9.88855958e-01],\n",
       "       [9.60446835e-01],\n",
       "       [4.65366244e-03],\n",
       "       [1.22246146e-03],\n",
       "       [2.23764777e-03],\n",
       "       [9.99207735e-01],\n",
       "       [9.97509599e-01],\n",
       "       [9.99988794e-01],\n",
       "       [9.62940037e-01],\n",
       "       [9.93010342e-01],\n",
       "       [9.92033601e-01],\n",
       "       [9.90615010e-01],\n",
       "       [9.92351651e-01],\n",
       "       [9.47294474e-01],\n",
       "       [9.99493599e-01],\n",
       "       [9.99376416e-01],\n",
       "       [9.24387693e-01],\n",
       "       [2.04509497e-03],\n",
       "       [9.12108243e-01],\n",
       "       [9.98628139e-01],\n",
       "       [9.91721392e-01],\n",
       "       [2.30866164e-01],\n",
       "       [1.22349262e-02],\n",
       "       [2.96235085e-05],\n",
       "       [0.00000000e+00],\n",
       "       [7.96616077e-05],\n",
       "       [1.09940767e-04],\n",
       "       [9.97774661e-01],\n",
       "       [8.26452136e-01],\n",
       "       [9.99741673e-01],\n",
       "       [9.99355078e-01],\n",
       "       [9.99269843e-01],\n",
       "       [9.79344010e-01],\n",
       "       [9.96770978e-01],\n",
       "       [6.25848770e-07],\n",
       "       [5.57303429e-06],\n",
       "       [7.35612869e-01],\n",
       "       [9.81622756e-01],\n",
       "       [9.98768687e-01],\n",
       "       [9.90014434e-01],\n",
       "       [8.67370341e-04],\n",
       "       [7.53786415e-03],\n",
       "       [2.82606652e-05],\n",
       "       [9.99959469e-01],\n",
       "       [8.26021889e-04],\n",
       "       [9.57057834e-01],\n",
       "       [2.32029834e-05]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=Model2.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_y=(y_pred>0.5)\n",
    "Pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_Y=Pred_y.reshape(143,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     True\n",
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "..    ...\n",
       "138     0\n",
       "139     1\n",
       "140     0\n",
       "141     1\n",
       "142     0\n",
       "\n",
       "[143 rows x 1 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Prediction=pd.get_dummies(Pred_Y,drop_first=True)\n",
    "Y_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54  1]\n",
      " [ 2 86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        55\n",
      "           1       0.99      0.98      0.98        88\n",
      "\n",
      "    accuracy                           0.98       143\n",
      "   macro avg       0.98      0.98      0.98       143\n",
      "weighted avg       0.98      0.98      0.98       143\n",
      "\n",
      "0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,Y_Prediction))\n",
    "print(classification_report(y_test,Y_Prediction))\n",
    "print(accuracy_score(y_test,Y_Prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54  1]\n",
      " [ 2 86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        55\n",
      "           1       0.99      0.98      0.98        88\n",
      "\n",
      "    accuracy                           0.98       143\n",
      "   macro avg       0.98      0.98      0.98       143\n",
      "weighted avg       0.98      0.98      0.98       143\n",
      "\n",
      "0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,Pred_y))\n",
    "print(classification_report(y_test,Pred_y))\n",
    "print(accuracy_score(y_test,Pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
